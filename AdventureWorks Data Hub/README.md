# ğŸš€ AdventureWorks Data Hub: End-to-End Azure Data Pipeline  

## ğŸ“Œ Overview  
This project demonstrates an **end-to-end Data Engineering pipeline** on **Azure** using the **AdventureWorks dataset**. It integrates **Azure Data Factory, Azure Data Lake, Azure Databricks, and Azure Synapse Analytics** to build a **scalable ETL workflow** for data ingestion, transformation, and reporting.  

---

## **ğŸ›  Architecture Diagram**  
![Project Architecture](./Images/Architecture.png)  

---

## **ğŸ“‚ Dataset**  
Dataset used: [AdventureWorks Dataset](https://www.kaggle.com/datasets/ukveteran/adventure-works)  
It includes the following CSV files:  
ğŸ“Œ **Calendar, Customers, Products, ProductCategories, ProductSubcategories, Returns, Sales_2015, Sales_2016, Sales_2017, Territories**  

---

## **ğŸ“Œ Steps Implemented in the Project**  

### **1ï¸âƒ£ Azure Resource Setup**  
ğŸ”¹ Created an **Azure Resource Group** for the project.  
ğŸ”¹ Provisioned required **Azure services**:  
   - **Azure Storage Account (Data Lake Gen2)**
   - **Azure Data Factory (ADF)**
   - **Azure Databricks**
   - **Azure Synapse Analytics**  

---

### **2ï¸âƒ£ Data Ingestion (Bronze Layer)**
ğŸ”¹ Configured **Azure Data Lake Gen2** and created three containers:  
   - **Bronze** â†’ Raw Data  
   - **Silver** â†’ Processed Data  
   - **Gold** â†’ Ready-to-serve Data  

ğŸ”¹ **Azure Data Factory (ADF) Pipelines**:
   - Extracted dataset from GitHub to the Bronze Layer dynamically using HTTP API.  
   - Implemented **Lookup + ForEach + Copy Data** activities.  
   - Configured [JSON](./Scripts/DynamicRawDataLoad.json) with relative paths and filenames to automate ingestion.  

---

### **3ï¸âƒ£ Data Transformation (Silver Layer - Azure Databricks)**
ğŸ”¹ Created an Azure Databricks Workspace and configured:  
   - IAM Role as Storage Blob Data Contributor for accessing the Data Lake.  
   - Databricks Compute Cluster & Notebook for processing raw data using PySpark.  

ğŸ”¹ **Transformation Steps (PySpark - Databricks Notebook)**
   - Read raw data from Bronze Layer.  
   - Applied **data cleaning, type conversion, and feature engineering**.  
   - Saved transformed data as **Parquet files** in the Silver Layer.
   ##### ğŸ“Œ **Refer to [Notebook](./Scripts/aw_silver_layer.ipynb).**


ğŸ“Œ **Example Transformation (Calendar Table)**  

```python
df_cal = spark.read.format("csv")\
        .option("header", "true")\
        .option("inferSchema",True)\
        .load("abfss://bronze@awdatalakeafreen.dfs.core.windows.net/Calendar")

df_cal = df_cal.withColumn("Month", month(col("Date")))\
               .withColumn("Year", year(col("Date")))

df_cal.write.format('parquet')\
            .mode('append')\
            .option("path","abfss://silver@awdatalakeafreen.dfs.core.windows.net/Calendar")\
            .save()
```
---

### **4ï¸âƒ£ Data Serving (Gold Layer - Azure Synapse)**  
ğŸ”¹ Connected Azure Synapse Analytics to the Silver Layer using Managed Identity (IAM).  
ğŸ”¹ Created Schemas, Views, and External Tables to serve the data efficiently.  
   ##### ğŸ“Œ **Refer to [create_view_gold.sql](./Scripts/create_view_gold.sql) and [external_table.sql](./Scripts/external_table.sql) for Views and External Tables.**  

---

### **5ï¸âƒ£ Reporting (Power BI)**  
ğŸ”¹ Connected Power BI to Azure Synapse for interactive analytics.  
ğŸ”¹ Created dashboards & visualizations to analyze sales trends, customer behavior, and product performance.  

---

## **ğŸ“Œ Technologies Used**  
âœ… **Cloud:** Azure (Data Factory, Synapse, Databricks, Storage)  
âœ… **ETL & Orchestration:** Azure Data Factory, Apache Airflow  
âœ… **Data Processing:** PySpark, SQL  
âœ… **Data Storage:** Azure Data Lake Gen2 (Bronze, Silver, Gold layers)  
âœ… **Data Warehousing:** Azure Synapse Analytics  
âœ… **Visualization:** Power BI  

---

### **ğŸš€ Conclusion**  
This project demonstrates an enterprise-level Data Engineering pipeline, leveraging Azure services for efficient data ingestion, transformation, storage, and analytics. It showcases end-to-end automation and scalability in cloud-based data pipelines.
